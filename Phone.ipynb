{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"phone_usage_india.csv\")\n",
    "train_data.rename(columns={\"Data Usage (GB/month)\":\"data_usage(gb/mo)\",\"Calls Duration (mins/day)\":\"calls_dur(min/d)\",\"Number of Apps Installed\":\"installed_apps\",\"Social Media Time (hrs/day)\":\"soical_media(h/d)\",\"E-commerce Spend (INR/month)\":\"E-commerce(INR/m)\",\"Streaming Time (hrs/day)\":\"streaming(hrs/d)\",\"Screen Time (hrs/day)\":\"Scr_time(h/d)\",\"Gaming Time (hrs/day)\":\"gaming(hrs/d)\",\"Monthly Recharge Cost (INR)\":\"recharge(INR)\"}, inplace=True)\n",
    "train_data.drop(columns=[\"User ID\"], axis=1, inplace=True)\n",
    "# Function to correct OS\n",
    "def fix_os(brand):\n",
    "    return \"iOS\" if brand == \"Apple\" else \"Android\"\n",
    "\n",
    "# Apply function to create a clean OS column\n",
    "train_data[\"OS\"] = train_data[\"Phone Brand\"].apply(fix_os)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(train_data)\n",
    "train_data.drop_duplicates(inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "train_data.columns\n",
    "# Define numerical columns to check for outliers\n",
    "num_cols = ['Scr_time(h/d)', 'data_usage(gb/mo)', 'calls_dur(min/d)', 'installed_apps',\n",
    "            'soical_media(h/d)', 'E-commerce(INR/m)', 'streaming(hrs/d)', 'gaming(hrs/d)', 'recharge(INR)']\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_iqr(train_data, cols):\n",
    "    for col in cols:\n",
    "        Q1 = train_data[col].quantile(0.25)  # First quartile (25th percentile)\n",
    "        Q3 = train_data[col].quantile(0.75)  # Third quartile (75th percentile)\n",
    "        IQR = Q3 - Q1  # Interquartile Range\n",
    "        lower_bound = Q1 - 1.5 * IQR  # Lower bound\n",
    "        upper_bound = Q3 + 1.5 * IQR  # Upper bound\n",
    "        \n",
    "        # Remove outliers\n",
    "        train_data = train_data[(train_data[col] >= lower_bound) & (train_data[col] <= upper_bound)]\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "train_data = remove_outliers_iqr(train_data, num_cols)\n",
    "train_data.head(15)\n",
    "train_data['Scr_time(min/d)'] = (train_data['Scr_time(h/d)'] * 60).astype(int)\n",
    "train_data['soical_media(min/d)'] = (train_data['soical_media(h/d)'] * 60).astype(int)\n",
    "train_data['streaming(min/d)'] = (train_data['streaming(hrs/d)'] * 60).astype(int)\n",
    "train_data['gaming(min/d)'] = (train_data['gaming(hrs/d)'] * 60).astype(int)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Bar plot: Primary Use vs Phone Brand\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x=train_data[\"Phone Brand\"], hue=train_data[\"Primary Use\"], palette=\"Set2\")\n",
    "plt.xlabel(\"Phone Brand\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Primary Use by Phone Brand\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Primary Use\")\n",
    "plt.show()\n",
    "apple_samsung_df = train_data[train_data[\"Phone Brand\"].isin([\"Apple\", \"Samsung\"])]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=apple_samsung_df[\"Location\"], hue=apple_samsung_df[\"Phone Brand\"], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Location\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Apple and Samsung Users by Location\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Phone Brand\")\n",
    "plt.show()\n",
    "phone_brand_counts = train_data[\"Primary Use\"].value_counts()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(phone_brand_counts, labels=phone_brand_counts.index, autopct='%1.1f%%', colors=sns.color_palette(\"pastel\"))\n",
    "plt.title(\"Primary Use Distribution\")\n",
    "plt.show()\n",
    "\n",
    "phone_brand_counts = train_data[\"OS\"].value_counts()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(phone_brand_counts, labels=phone_brand_counts.index, autopct='%1.1f%%', colors=sns.color_palette(\"pastel\"))\n",
    "plt.title(\"Operating System Distribution\")\n",
    "plt.show()\n",
    "# Bar plot: Average Screen Time by Gender\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=train_data[\"Gender\"], y=train_data[\"Scr_time(h/d)\"], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Average Screen Time (hrs/day)\")\n",
    "plt.title(\"Average Screen Time by Gender\")\n",
    "plt.show()\n",
    "\n",
    "train_data.drop(columns=[\"Scr_time(h/d)\",\"soical_media(h/d)\",\"streaming(hrs/d)\",\"gaming(hrs/d)\"], axis=1, inplace=True)\n",
    "train_data.replace({\"Female\":1,\"Male\":2,\"Other\":3}, inplace=True)\n",
    "train_data.replace({\"Android\":1,\"iOS\":0}, inplace=True)\n",
    "train_data = train_data.join(pd.get_dummies(train_data[\"Location\"], prefix=\"location\", drop_first=True, dtype=int))\n",
    "train_data = train_data.join(pd.get_dummies(train_data[\"Phone Brand\"], prefix=\"Phone_Brand\", drop_first=True, dtype=int))\n",
    "train_data.drop(columns=[\"Location\",\"Phone Brand\"], inplace=True)\n",
    "train_data[\"Primary Use\"].value_counts(normalize=True)\n",
    "mapping = {'Education': 1, 'Gaming': 2, 'Work': 3, \"Social Media\": 4, \"Entertainment\": 5}\n",
    "train_data['Primary Use'] = train_data['Primary Use'].map(mapping)\n",
    "train_data.columns\n",
    "train_data['calls_dur(min/d)'] = train_data['calls_dur(min/d)'].round().astype(int)\n",
    "train_data['data_usage(gb/mo)'] = train_data['data_usage(gb/mo)'].round().astype(int)\n",
    "data_train = train_data\n",
    "data_train\n",
    "import pandas as pd\n",
    "test_data = pd.read_csv(\"phone_usage_india.csv\")\n",
    "test_data.rename(columns={\"Data Usage (GB/month)\":\"data_usage(gb/mo)\",\"Calls Duration (mins/day)\":\"calls_dur(min/d)\",\"Number of Apps Installed\":\"installed_apps\",\"Social Media Time (hrs/day)\":\"soical_media(h/d)\",\"E-commerce Spend (INR/month)\":\"E-commerce(INR/m)\",\"Streaming Time (hrs/day)\":\"streaming(hrs/d)\",\"Screen Time (hrs/day)\":\"Scr_time(h/d)\",\"Gaming Time (hrs/day)\":\"gaming(hrs/d)\",\"Monthly Recharge Cost (INR)\":\"recharge(INR)\"}, inplace=True)\n",
    "test_data.drop(columns=[\"User ID\"], axis=1, inplace=True)\n",
    "# Function to correct OS\n",
    "def fix_os(brand):\n",
    "    return \"iOS\" if brand == \"Apple\" else \"Android\"\n",
    "\n",
    "# Apply function to create a clean OS column\n",
    "test_data[\"OS\"] = test_data[\"Phone Brand\"].apply(fix_os)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(test_data)\n",
    "test_data.dropna()\n",
    "test_data.columns\n",
    "\n",
    "# Define numerical columns to check for outliers\n",
    "num_cols = ['Scr_time(h/d)', 'data_usage(gb/mo)', 'calls_dur(min/d)', 'installed_apps',\n",
    "            'soical_media(h/d)', 'E-commerce(INR/m)', 'streaming(hrs/d)', 'gaming(hrs/d)', 'recharge(INR)']\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_iqr(test_data, cols):\n",
    "    for col in cols:\n",
    "        Q1 = test_data[col].quantile(0.25)  # First quartile (25th percentile)\n",
    "        Q3 = test_data[col].quantile(0.75)  # Third quartile (75th percentile)\n",
    "        IQR = Q3 - Q1  # Interquartile Range\n",
    "        lower_bound = Q1 - 1.5 * IQR  # Lower bound\n",
    "        upper_bound = Q3 + 1.5 * IQR  # Upper bound\n",
    "        \n",
    "        # Remove outliers\n",
    "        test_data = test_data[(test_data[col] >= lower_bound) & (test_data[col] <= upper_bound)]\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "test_data = remove_outliers_iqr(test_data, num_cols)\n",
    "test_data['Scr_time(min/d)'] = (test_data['Scr_time(h/d)'] * 60).astype(int)\n",
    "test_data['soical_media(min/d)'] = (test_data['soical_media(h/d)'] * 60).astype(int)\n",
    "test_data['streaming(min/d)'] = (test_data['streaming(hrs/d)'] * 60).astype(int)\n",
    "test_data['gaming(min/d)'] = (test_data['gaming(hrs/d)'] * 60).astype(int)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "test_data.drop(columns=[\"Scr_time(h/d)\",\"soical_media(h/d)\",\"streaming(hrs/d)\",\"gaming(hrs/d)\"], axis=1, inplace=True)\n",
    "test_data.replace({\"Female\":1,\"Male\":0,\"Other\":2}, inplace=True)\n",
    "test_data.replace({\"Android\":1,\"iOS\":0}, inplace=True)\n",
    "test_data = test_data.join(pd.get_dummies(test_data[\"Location\"], prefix=\"location\", drop_first=True, dtype=int))\n",
    "test_data = test_data.join(pd.get_dummies(test_data[\"Phone Brand\"], prefix=\"Phone_Brand\", drop_first=True, dtype=int))\n",
    "test_data.drop(columns=[\"Location\",\"Phone Brand\"], inplace=True)\n",
    "test_data[\"Primary Use\"].value_counts(normalize=True)\n",
    "test_data.isnull().sum().sum()\n",
    "test_data.drop_duplicates(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "mapping = {'Education': 0, 'Gaming': 1, 'Work': 2, \"Social Media\": 3, \"Entertainment\": 4}\n",
    "test_data['Primary Use'] = test_data['Primary Use'].map(mapping)\n",
    "test_data.columns\n",
    "test_data['calls_dur(min/d)'] = test_data['calls_dur(min/d)'].round().astype(int)\n",
    "test_data['data_usage(gb/mo)'] = test_data['data_usage(gb/mo)'].round().astype(int)\n",
    "data_test = test_data\n",
    "data_test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X = data_train.drop(\"Primary Use\", axis=1)\n",
    "y = data_train[\"Primary Use\"]\n",
    "\n",
    "# #Finds correlation between Independent and dependent attributes\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize = (18,18))\n",
    "# sns.heatmap(data_train.corr(), annot = True, cmap = \"RdYlGn\")\n",
    "\n",
    "# plt.show()\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "selection = ExtraTreesClassifier()\n",
    "selection.fit(X,y)\n",
    "# plt.figure(figsize=(12,8))\n",
    "# feat_importance = pd.Series(selection.feature_importances_,index=X.columns)\n",
    "# feat_importance.nlargest(20).plot(kind='barh')\n",
    "# plt.show()\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.countplot(x=train_data[\"Primary Use\"])\n",
    "# plt.show()\n",
    "<!-- ## Fitting model using Random Forest -->\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=42)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Train initial model to get feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Convert to Pandas Series for better visualization\n",
    "important_features = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "# Display top important features\n",
    "print(\"Feature Importance:\\n\", important_features)\n",
    "# Define threshold (e.g., drop features with importance < 0.02)\n",
    "threshold = 0.02\n",
    "selected_features = important_features[important_features > threshold].index\n",
    "\n",
    "# Keep only important features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features out of {X_train.shape[1]}\")\n",
    "\n",
    "# Train Random Forest with selected features\n",
    "rf_selected = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_acc = rf_selected.score(X_train_selected, y_train)\n",
    "test_acc = rf_selected.score(X_test_selected, y_test)\n",
    "\n",
    "print(f\"Train Accuracy after feature selection: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy after feature selection: {test_acc:.4f}\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = best_rf.score(X_train, y_train)\n",
    "test_acc = best_rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Optimized Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Optimized Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import numpy as np\n",
    "\n",
    "# # Train a basic model and check feature importances\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# # Get feature importance\n",
    "# importances = rf.feature_importances_\n",
    "# feature_names = X.columns\n",
    "# important_features = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "# # Display top important features\n",
    "# print(important_features.head(10))\n",
    "\n",
    "# print(data_train.isnull().sum())  # Look for missing values\n",
    "print(data_train.describe())      # Check for outliers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_reg_cl = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_reg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "# feature_importance = pd.Series(model_reg_cl.feature_importances_, index=X_train.columns)\n",
    "# feature_importance.nlargest(10).plot(kind='barh')\n",
    "# plt.show()    \n",
    "model_reg_cl.score(X_train, y_train)\n",
    "model_reg_cl.score(X_test, y_test)\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],  # Number of trees\n",
    "#     'max_depth': [10, 20, 30, None],  # Depth of trees\n",
    "#     'min_samples_split': [2, 5, 10],  # Minimum samples to split\n",
    "#     'min_samples_leaf': [1, 2, 4],  # Minimum samples per leaf\n",
    "#     'max_features': ['sqrt', 'log2'],  # Number of features per split\n",
    "#     'bootstrap': [True, False]  # Whether to use bootstrapping\n",
    "# }\n",
    "\n",
    "# # Initialize Random Forest\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Grid Search\n",
    "# grid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print best parameters\n",
    "# print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "# best_model = grid_search.best_estimator_  # If using GridSearchCV\n",
    "# # best_model = random_search.best_estimator_  # If using RandomizedSearchCV\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Predictions\n",
    "# y_train_pred = best_model.predict(X_train)\n",
    "# y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# # Accuracy scores\n",
    "# train_score = accuracy_score(y_train, y_train_pred)\n",
    "# test_score = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training Accuracy: {train_score:.4f}\")\n",
    "# print(f\"Testing Accuracy: {test_score:.4f}\")\n",
    "\n",
    "## Logistic Regression classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Initialize and train model\n",
    "log_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = log_model.predict(X_train)\n",
    "y_test_pred = log_model.predict(X_test)\n",
    "# Accuracy\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train the model\n",
    "dt_model = DecisionTreeClassifier(criterion='gini', max_depth=10, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = dt_model.predict(X_train)\n",
    "y_test_pred = dt_model.predict(X_test)\n",
    "# Initialize and train model\n",
    "log_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = log_model.predict(X_train)\n",
    "y_test_pred = log_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "# Accuracy\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1],\n",
    "    'colsample_bytree': [0.8, 1]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(XGBClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train best model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate new model\n",
    "print(\"Improved Test Accuracy:\", accuracy_score(y_test, best_xgb.predict(X_test)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
